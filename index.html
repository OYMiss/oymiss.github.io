<!DOCTYPE html><html><head><meta charset="utf-8"><title>Home</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<link rel="stylesheet" href="/style/style.css"></head><body>
<header class="header" id="header">
    <div class="header-wrapper">
        <div class="logo">
            <h1><a href="/">OY's Home</a></h1>
        </div>
        <nav class="main-nav">
            <ul class="menu">
                <li class="menu-item">
                    <a href="/" id="home">
                        <span class="base-name">
                            Home
                        </span>
                    </a>
                </li>
                <li class="menu-item">
                    <a href="/archives" id="archives">
                        <span class="base-name">
                            Archives
                        </span>
                    </a>
                </li>
                <li class="menu-item">
                    <a href="/categories" id="categories">
                        <span class="base-name">
                            Categories
                        </span>
                    </a>
                </li>
                <li class="menu-item">
                    <a href="/about" id="about">
                        <span class="base-name">
                            About
                        </span>
                    </a>
                </li>
            </ul>
        </nav>
    </div>
</header>
<article class="homepage-body"><div class="markdown-body limit-height"><h1>小米 4A 千兆版刷机</h1><h2>软件刷机</h2><p>教程可以参考这个 <a href="https://www.right.com.cn/forum/thread-4007071-1-1.html">小米4A千兆版 NO拆机</a></p><p>但是我一直无法访问进去，telnet 192.168.1.1 一直是拒绝访问。原因是 GitHub 在国内访问不稳定，该下载东西无法下载。</p><h3>预先下载文件</h3><p>下载好这两个文件，<a href="https://github.com/acecilia/OpenWRTInvasion/raw/master/script_tools/dropbearStaticMipsel.tar.bz2">dropbearStaticMipsel.tar.bz2</a>、<a href="https://github.com/acecilia/OpenWRTInvasion/raw/master/script_tools/busybox-mipsel">busybox-mipsel</a></p><h3>建立文件共享服务器</h3><p>如果是 macOS：</p><pre><code># cd 到文件所在目录运行这个，比如 cd ~/Download
sudo python -m SimpleHTTPServer 80 
</code></pre><p>如果是 Windows：</p><p>据说可以用 <a href="http://iscute.cn/tar/chfs/2.0/gui-chfs-windows.zip">chfs</a> 来实现类似功能</p><h3>修改 <code>script.sh</code> 文件</h3><p>假设电脑的 ip 是 192.168.31.8，将 OpenWRTInvasion 里面的 <code>script.sh</code> 中 <code>https://github.com/acecilia/OpenWRTInvasion/raw/master/script_tools/</code> 改成 <code>http://192.168.31.8/</code>。</p><p>里面代码变成这样。</p><pre><code>curl -L "http://192.168.31.8/busybox-mipsel" --insecure --output busybox
curl -L "http://192.168.31.8/dropbearStaticMipsel.tar.bz2" --output dropbear.tar.bz2
</code></pre><h3>执行</h3><p>然后再执行 <code>python remote_command_execution_vulnerability.py</code>，这时候再 telnet 就可以了。</p><h2>编程器刷机（救砖）</h2><p>不小心路由器成砖了。于是买编程器进行刷机。（之前备份过 eeprom）</p><p>拆开路由器（螺丝在贴纸下面），找到 Flash。</p><blockquote><p>芯片是分正反的，芯片上右上角有一个凹槽，来指示位置。</p></blockquote><p><img src="/asset/xiaomi-4a-gigabit-flash-rom/flash.png" width = "60%" height = "60%" alt="flash"/></p><p>淘宝买了 CH341A 带夹子的那种。</p><ol><li>按照下图红框内的图示安装好这个芯片，小米 4A 是 25 系芯片，所以安装在图中下面的槽中。</li></ol><p><img src="/asset/xiaomi-4a-gigabit-flash-rom/programmer.png" width = "60%" height = "60%" alt="programmer"/></p><p>圈住的部分对应芯片上的凹槽位置，用来确定正反。</p><p><img src="/asset/xiaomi-4a-gigabit-flash-rom/1position.png" width = "60%" height = "60%" alt="1position"/></p><ol><li>按照图示安装架子。</li></ol><p><img src="/asset/xiaomi-4a-gigabit-flash-rom/connect.png" width = "60%" height = "60%" alt="connect"/></p><ol><li>用夹子夹住Flash芯片，红线应该对应芯片上凹槽所在的位置。</li></ol><p>连接到电脑上（据说要连到 USB3.0接口上）</p><ol><li>利用 Windows 虚拟机刷到一半刷机程序未响应，所以直接用命令行进行烧录。</li></ol><p>我用的 macOS，如果是 Linux 应该也一样。先去下载 <a href="https://breed.hackpascal.net/breed-mt7621-pbr-m1.bin">breed-mt7621-pbr-m1.bin</a>。</p><pre><code># brew install truncate
# brew install flashrom

truncate -s +16670883 breed-mt7621-pbr-m1.bin
flashrom --programmer ch341a_spi -c "W25Q128.V" --write breed-mt7621-pbr-m1.bin
</code></pre><p>运行完命令，breed 就刷好了。救砖成功！</p><p>[1] <a href="https://www.right.com.cn/forum/thread-4007071-1-1.html">小米4A千兆版 NO拆机</a></p><p>[2] <a href="https://forum.openwrt.org/t/xiaomi-wifi-router-3g-v2/42584/46">Xiaomi WiFi Router 3G V2</a></p><p>[3] <a href="https://www.right.com.cn/forum/thread-3161868-1-1.html">小米路由器4A千兆版（R4A）之编程器刷机</a></p><p>[4] <a href="https://www.right.com.cn/forum/thread-3908050-1-1.html">小米路由器4A千兆版编程器刷入PandoraBox</a></p><a class="fill-link" href="/misc/xiaomi-4a-gigabit-flash-rom"></a><div class="hide-article-box"></div></div><div class="markdown-body limit-height"><h1>常见层结构</h1><p>所有向量默认指的是列向量，并且记 \(A_i \in \mathbb R^m\) 表示矩阵 \(A\in \mathbb R^{m \times n}\) 的第 \(i\) 个列向量，第 \(j\) 个行元素组成的向量（不是行向量，所以维度并不是 \(\mathbb R^{1\times n}\)）表示为 \((A^T)_j \in \mathbb R^n\)。</p><h3>Affine 层</h3><h4>公式</h4>\[\begin{aligned}
    Y &= XW + B \\
    Y_{i,l} &= \sum_k W_{k,l} X_{i,k} + b_l \\
\end{aligned}
\]<p>其中 \(X \in \mathbb R^{N \times D}\)，\(W \in \mathbb R^{D \times M}\)，\(B \in \mathbb R^{N \times M}\)，\(b \in \mathbb R^{M}\)，\(B =\begin{bmatrix}b^T \\\vdots \\b^T\end{bmatrix}\)，\(Y \in \mathbb R^{N \times M}\)。</p><p><!-- more --></p><h4>导数</h4><p>求对 \(b\) 的导数：</p>\[\frac{\partial Y_{i,l}}{\partial b_l} = 1
\]\[\begin{aligned}
\frac{\partial \ell}{\partial b_l} &=\sum_{i} \frac{\partial \ell}{\partial Y_{i,l}}\frac{\partial Y_{i,l}}{\partial b_l} \\
&= \sum_{i} dY_{i,l} \\
&= {dY}_{l} \cdot \begin{bmatrix} 1 \\ \vdots\\ 1 \end{bmatrix}
\end{aligned}
\]\[\frac{\partial \ell}{\partial b} = dY^T \begin{bmatrix}
1 \\
\vdots\\
1
\end{bmatrix} = (\begin{bmatrix}
1 & \cdots & 1
\end{bmatrix} dY)^T
\]<p>求对 \(W\) 的导数：</p>\[\frac{\partial Y_{i,l}}{\partial W_{k, l}} = X_{i,k}
\]\[\begin{aligned}
\frac{\partial \ell}{\partial W_{k,l}}
&= \sum_{i}\frac{\partial \ell}{\partial Y_{i,l}} \frac{\partial Y_{i,l}}{\partial W_{k, l}} \\
&= \sum_{i}\frac{\partial \ell}{\partial Y_{i,l}} X_{i,k} \\
&= X_{k} \cdot dY_{l}
\end{aligned}
\]\[\frac{\partial \ell}{\partial W} = X^TdY
\]<p>求对 \(X\) 的导数：</p>\[\frac{\partial Y_{i,l}}{\partial X_{i,k}} = W_{k,l}
\]\[\begin{aligned}
\frac{\partial \ell}{\partial X_{i, k} } &= \sum_l \frac{\partial \ell}{\partial Y_{i,l}} \frac{\partial Y_{i,l}}{\partial X_{i,k}} \\
&= \sum_l \frac{\partial \ell}{\partial Y_{i,l}} W_{k, l} \\
&= (dY^T)_i \cdot (W^T)_k
\end{aligned}
\]\[\frac{\partial \ell}{\partial X } = dY W^T
\]<h3>ReLU 层</h3><h4>公式</h4>\[\begin{aligned}
Y &= \max(0, X)\\
Y_{i, j} &= \max(0, X_{i, j})
\end{aligned}
\]<h4>导数</h4>\[\frac{\partial Y_{i,j}} {\partial X_{i, j}} = 1\{ X_{i, j} > 0 \}
\]\[\begin{aligned}
\frac{\partial \ell}{\partial X_{i, j}} &= \frac{\partial \ell}{\partial Y_{i,j}} \frac{\partial Y_{i,j}} {\partial X_{i, j}} \\
&= \frac{\partial \ell}{\partial Y_{i,j}}1\{ X_{i, j} > 0 \}\\
\end{aligned}
\]<h3>SVM 损失</h3><h4>公式</h4>\[\begin{aligned}
L &= \sum_i L_i \\
L_i &= \sum_{j\neq y_i} \max(0, x_j - x_{y_i} + \Delta) \\
\text{margin}_{i, j} &=
\begin{cases} 0 & \text{when $j = y_i$}\\
x_j - x_{y_i} + \Delta & \text{when $x_j - x_{y_i} + \Delta > 0$} \\
0 & \text{when $x_j - x_{y_i} + \Delta \le 0$}
\end{cases} \\
L &= \sum_{i, j} \text{margin}_{i, j}
\end{aligned}
\]<p>其中 \((X^T)_i = x\)，\(X\in \mathbb R^{N \times D}\)。</p><h4>导数</h4><p>先写出 \(\text{margin}\) 的导数：</p>\[\frac{\partial \text{margin}_{i,j}}{\partial x_k} =
\begin{cases}
1 & k = j \\
-1 & k = y_i \\
\end{cases} \text{ when margin}_{i,j} > 0 \\
\]<p>对于 \(k = y_i\) 的情况下：</p>\[\begin{aligned}
\frac{\partial L_i}{\partial X_{i,y_{i}}} &= \frac{\partial L_i}{\partial x_{y_i}} \\
&= \sum_j\frac{\partial \text{margin}_{i,j}}{\partial x_{y_i}} \\
&= -\sum_j 1\{\text{margin}_{i,j} > 0\}\\
\end{aligned} \\
\]<p>对于 \(k \ne y_i\) 的情况下：</p>\[\begin{aligned}
\frac{\partial L_i}{\partial X_{i,k}} &= \frac{\partial L_i}{\partial x_k} \\
&= \sum_j\frac{\partial \text{margin}_{i,j}}{\partial x_{k}} \\
&=  1\{\text{margin}_{i,k} > 0\} \\
\end{aligned} \\
\]<h3>Softmax 损失</h3><h4>公式</h4>\[\begin{aligned}
    L &= \sum_i L_i \\
    L_i &= -\log\left(\frac{e^{x_{y_i}}}{ \sum_j e^{x_j} }\right) \\
    L_i &= -x_{y_i} + \log\sum_j e^{x_j}
\end{aligned}
\]<p>其中 \((X^T)_i = x\)，\(X\in \mathbb R^{N \times D}\)。</p><h4>导数</h4>\[\frac{\partial L_i}{\partial X_{i,k}} = \frac{\partial L_i}{\partial x_k} = -1\{k = y_i\} + \frac{e^{x_k}}{\sum_j e^{x_j}}
\]<blockquote><p>计算出 \(e^{x_k} / \sum_j e^{x_j}\) 之后，就可以求出 \(\ell\) 和 \(dX\)。</p></blockquote><h3>Batch Normalization 层</h3><h4>公式</h4>\[\begin{aligned}
    \mu &= \frac{1}{m} \sum_{i=1}^m x_i \\
    \sigma^2 &= \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 \\
    \hat x_i &= \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \\
    y_i &= \gamma \hat x_i + \beta = \text{BN}_{\gamma, \beta}(x_i)
\end{aligned}
\]<p>需要注意，这里的要训练的参数为 \(\gamma\) 和 \(\beta\)。</p><h4>导数</h4>\[\begin{aligned}
\frac{\partial \ell}{\partial \hat x_i}
&= \frac{\partial \ell}{\partial y_i} \gamma \\
\frac{\partial \ell}{\partial \sigma^2} &= \sum_{i=1}^{m} \frac{\partial \ell}{\partial \hat x_i} (x_i - \mu)(-\frac{1}{2})(\sigma^2 + \epsilon)^{-\frac{3}{2}} \\
\frac{\partial \ell}{\partial \mu} &= \frac{\partial \ell}{\partial \sigma^2}(-\frac{2}{m} )\sum_{i=1}^m (x_i - \mu) + \sum_{i=1}^{m} \frac{\partial \ell}{\partial \hat x_i} \frac{-1}{\sqrt{\sigma^2 + \epsilon}} \\
\frac{\partial \ell}{\partial x_i} &= \frac{\partial \ell}{\partial \hat x_i} \frac{1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial \ell}{\partial \sigma^2} \frac{2}{m} (x_i - \mu) + \frac{\partial \ell}{\partial \mu} \frac{1}{m} \\
\frac{\partial \ell}{\partial \gamma} &= \sum_{i=1}^m \frac{\partial \ell}{\partial y_i}\hat x_i \\
\frac{\partial \ell}{\partial \beta} &= \sum_{i=1}^m \frac{\partial \ell}{\partial y_i}1 \\
\end{aligned}
\]<p>如果把 \(\text{BN}\) 层分为多层，然后计算每层的导数：</p>\[\begin{aligned}
\mu &= \frac{1}{m} \sum_{i=1}^m x_i \\
v &= \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 \\
\sigma &= \sqrt{v + \epsilon} \\
\hat x_i &= \frac{x_i - \mu}{\sigma} \\
\end{aligned}
\]<p>\(\hat x_i \to \sigma, \mu\) 过程：</p>\[\begin{aligned}
\frac{\partial \ell}{\partial \sigma} &= \sum_i \frac{\partial \ell}{\partial \hat x_i} \frac{ x_i - \mu }{-\sigma^2} \\
\frac{\partial \ell}{\partial \mu} &= \sum_i \frac{\partial \ell}{\partial \hat x_i} \frac{ -1 }{\sigma} \\
\frac{\partial \ell}{\partial x_i} &= \frac{\partial \ell}{\partial \hat x_i} \frac{ 1 }{\sigma} \\
\end{aligned}
\]<p>\(\sigma \to v\) 过程：</p>\[\frac{\partial \ell }{\partial v} = \frac{\partial \ell}{\partial \sigma} \frac{1}{2\sqrt{v + \epsilon}} = \frac{\partial \ell}{\partial \sigma} \frac{1}{2\sigma} \\
\]<p>\(v \to x, \mu\) 过程：</p>\[\begin{aligned}
\frac{\partial \ell}{\partial x_i} &= \frac{\partial \ell}{\partial v} \frac{2}{m} (x_i - \mu) \\
\frac{\partial \ell}{\partial \mu} &= \frac{\partial \ell}{\partial v} (-\frac{2}{m}) \sum_i (x_i - \mu) \\
\end{aligned}
\]<p>\(\mu \to x\) 过程：</p>\[\frac{\partial \ell }{\partial x_i} = \frac{\partial \ell }{\partial \mu} \frac{1}{m}
\]<h3>Dropout 层</h3><h4>公式</h4>\[y = x \frac{1\{a \ge p\}}{p}
\]<p>其中 \(a\) 是 \((0, 1)\) 的随机数。如果不除以 \(p\) 会导致输出的均值与输入均值相差 \(p\) 倍。</p><h4>导数</h4>\[\begin{aligned}
\frac{\partial \ell}{\partial x}
&= \frac{\partial \ell}{\partial y} \frac{\partial y}{\partial x} \\
&= \frac{\partial \ell }{\partial y} \frac{1\{a \ge p\}}{p}
\end{aligned}
\]<h3>卷积层</h3><h4>公式</h4>\[Y_{i,j, k, l} = \sum_c \sum_{a, b} X_{i,c,ks + a,ls + b} W_{j,c,a,b} + b_j
\]<p>\(s\) 为 \(W\) 移动的步长，其中 \(X \in \mathbb R^{N \times C \times H \times W}\)，\(W \in \mathbb R^{F \times C \times H_w \times W_w}\)，\(Y \in \mathbb R^{N \times C \times H' \times W'}\)，并有 \(H' = 1 + (H - H_w) / s, W' = 1 + (W - W_w) / s\)。</p><p>利用 <code>numpy</code> 的函数：</p>\[Y_{i,j,k,l} = \sum_c \text{np.sum}(X^{(k, l)}_{i,c} * W_{j,c}) + b_j
\]<blockquote><p>这里 \(*\) 是 element-wise multiplication。</p></blockquote><p>为了方便计算，令 \(X^{(k, l)}_{i,c,a,b} = X_{i,c,ks + a,ls + b}\)，这里 \(X^{(k, l)}_{i,c}\) 表示在矩阵 \(X_{i,c}\) 中左上角坐标为 \((ks, ls)\)，右下角坐标为 \((ks + H_w - 1, ls+W_w - 1)\) 的子矩阵。</p><p>因为输入数据为维度为 \(\mathbb R^{N \times C \times H \times W}\)，卷积层的 Normalization 也与通常的不太一样。</p><p>下图展示了四种 Normalization 对卷积层的处理方式：</p><p><img src="/asset/common-layers/all-norms.svg" width = "100%" height = "100%" alt="all-norms"/></p><blockquote><p>计算的技巧是将维度转化为二维，过程与 Batch Normalization 几乎一样。</p></blockquote><p>对于 CNN 来说，Group Normalization 既拥有 Layer Normalization 与 batch size 无关的性质又与  Batch Normalization 效果差不多。</p><h4>导数</h4><p>首先求各自偏导数：</p>\[\begin{aligned}
\frac{\partial Y_{i,j,k,l}}{\partial X^{(k, l)}_{i,c}} &= W_{j,c} \\
\frac{\partial Y_{i,j,k,l}}{\partial W_{j,c}} &= X^{(k, l)}_{i,c} \\
\frac{\partial Y_{i,j,k,l}}{\partial b_j} &= 1\\
\end{aligned}
\]<p>然后求损失函数的导数：</p>\[\begin{aligned}
\frac{\partial \ell}{\partial X^{(k, l)}_{i,c}}
&= \sum_j \frac{\partial \ell}{\partial Y_{i, j, k, l}} \frac{\partial Y_{i, j, k, l}}{\partial X^{(k, l)}_{i,c}} \\
&= \sum_j \frac{\partial \ell}{\partial Y_{i, j, k, l}} W_{j,c} \\
\frac{\partial \ell}{\partial W_{j,c}}
&= \sum_i \sum_{k,l} \frac{\partial \ell}{\partial Y_{i, j, k, l}} \frac{\partial Y_{i, j, k, l}}{\partial W_{j,c}} \\
&= \sum_i \sum_{k,l} \frac{\partial \ell}{\partial Y_{i, j, k, l}} X^{(k, l)}_{i,c} \\
\frac{\partial \ell}{\partial b_j}
&= \sum_i \sum_{k,l} \frac{\partial \ell}{\partial Y_{i, j, k, l}} \frac{\partial Y_{i, j, k, l}}{\partial b_{j}} \\
&= \sum_i \sum_{k,l} \frac{\partial \ell}{\partial Y_{i, j, k, l}}
\end{aligned}
\]<h3>Max-Pooling 层</h3><h4>公式</h4>\[Y_{i,c,k,l} =  \max_{a,b} X_{i,c,ks + a,ls + b}
\]<p>为了方便计算，令 \(X^{(k, l)}_{i,c,a,b} = X_{i,c,ks + a,ls + b}\)，这里 \(X^{(k, l)}_{i,c}\) 表示在矩阵 \(X_{i,c}\) 中左上角坐标为 \((ks, ls)\)，右下角坐标为 \((ks + H_p - 1, ls+W_p - 1)\) 的子矩阵。</p><h4>导数</h4>\[\begin{aligned}
\frac{\partial \ell}{\partial X^{(k, l)}_{i,c,a,b}}
&= \frac{\partial \ell}{\partial Y_{i, c, k, l}} \frac{\partial Y_{i, c, k, l}}{\partial X^{(k, l)}_{i,c,a,b}} \\
&= \frac{\partial \ell}{\partial Y_{i, j, k, l}} 1\{ (a,b) = \arg \max(X^{(k, l)}_{i,c})\}
\end{aligned}
\]<p>[1] <a href="http://cs231n.stanford.edu/">CS231n: Convolutional Neural Networks for Visual Recognition</a></p><p>[2] <a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p><p>[3] <a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></p><p>[4] <a href="https://arxiv.org/abs/1803.08494">Group Normalization</a></p><a class="fill-link" href="/ml/cs231n/common-layers"></a><div class="hide-article-box"></div></div><div class="markdown-body limit-height"><h1>用线性代数的眼光来看离散傅立叶变换</h1><p>如果 \(x\) 和 \(c\) 都是长度为 \(n\) 的向量，那么它们循环卷积的结果 \(y\) 就有</p>\[y_k = (x \otimes c)_k = \sum_{i=0}^{n-1} x_i c_{k-i}
\]<blockquote><p>其中 \(c_{k-i}\) 代表循环移位。</p></blockquote><p>将 \(c\) 的循环移位写成矩阵的形式</p>\[C=
\begin{bmatrix}
c_0     & c_{n-1} & \dots  & c_{2} & c_{1}  \\
c_{1} & c_0    & c_{n-1} &         & c_{2}  \\
\vdots  & c_{1}& c_0    & \ddots  & \vdots   \\
c_{n-2}  &        & \ddots & \ddots  & c_{n-1}   \\
c_{n-1}  & c_{n-2} & \dots  & c_{1} & c_0 \\
\end{bmatrix}
\]<p><!--more--></p><p>于是就有</p>\[y = Cx
\]<p>举个例子：</p>\[\begin{bmatrix}
16 \\
14 \\
16 \\
14 \\
\end{bmatrix}
=\begin{bmatrix}
1 & 4 & 3 & 2 \\
2 & 1 & 4 & 3 \\
3 & 2 & 1 & 4 \\
4 & 3 & 2 & 1 \\
\end{bmatrix}
\begin{bmatrix}
1 \\
2 \\
1 \\
2 \\
\end{bmatrix}
\]<p>其中 \(C\) 可以用另一种方式表示：</p>\[C=f(P)=c_0I+c_1P+c_2P^2+\ldots+c_{n-1}P^{n-1}
\]<p>\(P\) 矩阵是一个置换矩阵，也被称作循环矩阵。</p>\[P=
\begin{bmatrix}
 0&0&\ldots&0&1\\
 1&0&\ldots&0&0\\
 0&\ddots&\ddots&\vdots&\vdots\\
 \vdots&\ddots&\ddots&0&0\\
 0&\ldots&0&1&0
\end{bmatrix}
\]<p>它可以表示为</p>\[\begin{pmatrix}
   0 & I_{n-k} \\
   I_k & 0
\end{pmatrix}
\]<p>可以得到 \(\vert P - \lambda I_n\vert = \lambda^n - 1\)，也可以通过 \(P^n = I\) 来得到 \(\lambda^n = 1\)。根据 \(e^{-j2\pi k} = 1\)，解出来得到 \(\lambda_k = e^{\frac{-j2\pi k}{n}} = \omega^k\)，计算可以得到 \(P\alpha_k = \omega^k(\alpha_{k(n-1)}, \alpha_{k(0)}, \cdots, \alpha_{k(n-3)}, \alpha_{k(n-2)})^T\)。</p><blockquote><p>这里不明白 \(\omega = e^{-j2\pi/n}\)，还是 \(\omega = e^{j2\pi/n}\)，感觉是一样的。</p></blockquote><p>从而有 \(\alpha_{k(i)} = \omega^k\alpha_{k(i-1)}\) 前一项乘以 \(\omega^{k}\) 等于后一项，最后一项乘以 \(\omega^{k}\) 得到第一项。如果 \(\alpha_{k} = (1, \omega^{k}, \omega^{2k}, \cdots, \omega^{(n-1)k})^T\) 就正好满足，其中有 \(\omega^{kn}=\omega^{k0}=1\) 。</p><p>可以得到 \(P\) 的特征向量组成的矩阵：</p>\[F = \begin{bmatrix}
1&1&1&1&\cdots &1 \\
1&\omega&\omega^2&\omega^3&\cdots&\omega^{n-1} \\
1&\omega^2&\omega^4&\omega^6&\cdots&\omega^{2(n-1)}\\ 1&\omega^3&\omega^6&\omega^9&\cdots&\omega^{3(n-1)}\\
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\
1&\omega^{n-1}&\omega^{2(n-1)}&\omega^{3(n-1)}&\cdots&\omega^{(n-1)(n-1)}
\end{bmatrix}
\]<p>\(C\) 矩阵的特征向量也是 \(\alpha_k\)，但是特征值变为</p>\[\begin{aligned}
\lambda_k^* &= c_0 + c_1\lambda_k + c_2\lambda_k^2 + \cdots + c_{n-1}\lambda_k^{n-1} \\
&= (c_0, c_1, \cdots, c_{n-1})^T(1, \lambda_k^1, \lambda_k^2, \cdots, \lambda_k^{n-1}) \\
&= (c_0, c_1, \cdots, c_{n-1})^T(1, \omega^{k}, \omega^{2k}, \cdots, \omega^{(n-1)k}) \\
&= (c_0, c_1, \cdots, c_{n-1})^T\alpha_{k}
\end{aligned}
\]<p>可以得到特征值组成的向量为 \(Fc\)，将 \(C\) 对角化，得到：</p>\[C = F^{-1}diag(Fc)F
\]<p>这时候如果将 \(y = Cx\) 中的 \(x\) 投影到 \(F\) 列（行）空间里，也就是 \(C\) 的特征向量张成的空间，可以得到 \(y\) 在这个空间中的坐标。</p><p>如果将 \(x\) 和 \(y\) 也写成类似于 \(C\) 的循环矩阵，可以得到：</p>\[Y = CX
\]<p>然后做对角化可以得到：</p>\[\begin{aligned}
F^{-1}diag(Fy)F
&= F^{-1}diag(Fc)F F^{-1}diag(Fx)F \\
&= F^{-1}diag(Fc)diag(Fx)F
\end{aligned}
\]<p>于是可以得到卷积定理：</p>\[\begin{aligned}
Fy
&= Fc \times Fx \\
&= F(c \otimes x)
\end{aligned}
\]<blockquote><p>这里 \(\times\) 表示 element-wise multiplication。</p></blockquote><p>这就是著名的卷积定理，时域卷积对应着频域的乘积，而频域是向量构成的循环矩阵的特征值。</p><p>[1] <a href="https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/">Matrix Methods in Data Analysis, Signal Processing, and Machine Learning</a></p><p>[2] <a href="https://en.wikipedia.org/wiki/Circulant_matrix">Circulant matrix</a></p><p>[3] <a href="https://en.wikipedia.org/wiki/Discrete_Fourier_transform">Discrete Fourier transform</a></p><p>[4] <a href="https://en.wikipedia.org/wiki/Circular_convolution">Circular convolution</a></p><a class="fill-link" href="/signal/explain-dft-by-linear-algebra"></a><div class="hide-article-box"></div></div><div class="markdown-body limit-height"><h1>矩阵的广义逆</h1><h3>定义</h3><p>对于一个 \(m \times n\) 的实矩阵 \(A\)，\(A^+\) 应该满足下面四个条件：</p><ol><li>\(AA^+A = A\)</li><li>\(A^+AA^= A^+\)</li><li>\((AA^+)^T = AA^+\)</li><li>\((A^+A)^T = A^+A\)</li></ol><p>如果 \(A\) 列线性无关，则有：</p>\[A^+ = (A^TA)^{-1}A^T
\]<p>称为左逆，并有 \(A^+A = I\)。</p><p><!--more--></p><p>如果 \(A\) 行线性无关，则有：</p>\[A^+ = A^T(AA^T)^{-1}
\]<p>称为右逆，并有 \(AA^+ = I\)。</p><h3>几何解释</h3><p>对于一个线性方程组 \(Ax = y\)，\(A_{m \times n}\) 矩阵是一个 \(\mathbb{R}^n \to \mathbb{R}^m\) 的线性变换。需要解出 \(x^* \in \mathbb{R}^n\) 使得 \(Ax^* = y\)，可以通过广义逆求出 \(x^* = A^+y\)，相当于 \(x^* = A^+Ax^*\)，这里的 \(Ax^*\) 也一定在 \(A\) 的列空间中。</p><p>如果方程 \(Ax = y\) 无解，即找不到一个 \(x^*\) 使得 \(Ax^* = y\)，将 \(y\) 投影到 \(A\) 的列空间上并得到 \(y'\)，这样方程才有解，即 \(Ax^* = y'\)。</p><p>根据 \(A\) 的向量子空间的性质，\(C(A)\) 与 \(N(A^T)\) 是正交的，可以用这两个空间里面的向量张成整个 \(\mathbb{R}^m\)。一定能将 \(y_m\) 分解为 \(y_m = P_{C(A)}(y) + P_{N(A^T)}(y) = y' + y_0\)。可以得到</p>\[A^Ty = A^Ty' + A^Ty_0 = A^Ty'
\]<p>其中 \(y'\) 可以写成 \(Ax^*\)，可以得到 \(A^Ty = A^TAx^*\)，如果 \(A^TA\) 可以逆，即 \(R(A) = n\)，则有</p>\[x^* = (A^TA)^{-1}A^Ty
\]<p>其中，\((A^TA)^{-1}A^T\) 叫做 Moore-Penrose 广义逆。</p><h3>求解</h3><blockquote><p>如果 \(A^{-1}\) 存在，那么 \(A^+ = A^{-1}\)。</p></blockquote><p>将 \(A\) 进行 SVD 分解即 \(A = U\Sigma V^T\)，就有 \(A^+ = V\Sigma^+U^T\)。</p><p>其中 \(\Sigma^+\) 就是将 \(\Sigma\) 转置一下然后将 \(\sigma_i\) 变为 \(1/\sigma_i\)。</p><p>[1] <a href="https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/">Matrix Methods in Data Analysis, Signal Processing, and Machine Learning</a></p><p>[2] <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore–Penrose inverse</a></p><p>[3] <a href="http://massimozanetti.altervista.org/files/mydocs/geometricalMeaningMoorePenrose.pdf">Geometrical meaning of the Moore-Penrose pseudo inverse</a></p><a class="fill-link" href="/math/pseudoinverse"></a><div class="hide-article-box"></div></div><div class="markdown-body limit-height"><h1>奇异值分解与主成分分析</h1><h3>奇异值分解（SVD）</h3><p>如果矩阵 \(A_{m\times n}\) 不能够进行特征值分解，要考虑使用<strong>奇异值分解（singular value decomposition，SVD）</strong> 化成：</p>\[A = U\Sigma V^T
\]<p>其中，\(U_{m\times m}\) 和 \(V_{n\times n}\) 都是正交矩阵，\(\Sigma_{m\times n}\) 是一个矩形对角矩阵 （rectangular diagonal matrices），并且 \(\Sigma\) 主对角线上的元素都是非负的，记作 \(\sigma_i\)。</p>\[AA^T = U\Sigma V^T V\Sigma^T U^T = U \Sigma\Sigma^T U^T
\]<p>其中 \(AA^T\) 是对称矩阵，一定能够对角化。</p><p><!--more--></p><p>\(U\) 被称作 \(A\) 的<strong>左奇异向量 （left-singular vectors）</strong>，它是 \(AA^T\) 的特征向量的一个集合。</p><p>\(V\) 被称作 \(A\) 的<strong>右奇异向量 （right-singular vectors）</strong>，它是 \(A^TA\) 的特征向量的一个集合。</p><p>\(AA^T\) 与 \(A^TA\) 拥有同样的特征值，于是 \(\sigma_i^2\) 是 \(AA^T\) 的特征值。</p><p>相对于特征值分解中 \(Sv_i = \lambda v_i\)，奇异值分解有：</p>\[Av_i = \sigma_i u_i \qquad i = 1, \ldots, \min(m, n)
\]<p>可以把矩阵 \(A\) 看作是一个变换，将 \(\mathbb{R}^m\) 变换到 \(\mathbb{R}^n\)，其中 \(\mathbb{R}^m\) 的基向量为 \(u_i\)，\(\mathbb{R}^n\) 的基向量为 \(v_i\)。</p><h3>主成分分析（PCA）</h3><p><strong>主成分分析（Principal components analysis，PCA）</strong> 是一种统计分析、简化数据集的方法。它利用正交变换来对一系列可能相关的变量的观测值进行线性变换，从而投影为一系列线性不相关变量的值，这些不相关变量称为主成分（Principal Components）。</p><p>用矩阵表示就是</p>\[T = XW
\]<p>把数据变换到新的坐标系统后，使得第一主成分描述的是最大方差的方向，第二主成分是第二大方差的方向，以此类推。</p><p>对于每个主成分有：\(t_{k(i)} = x_{i} \cdot w_{(k)}\)。</p><p>为了使得方差最大化，第一个权重向量 \(w_1\) 应该满足：</p>\[\begin{align*} \\
 w_{(1)}
 & = \underset{\Vert \mathbf{w} \Vert = 1}{\arg \max} \{ \sum(t_1)^2_{(i)}\} \\
 & = \underset{\Vert \mathbf{w} \Vert = 1}{\arg \max} \{ \sum(x_{(i)}\cdot w)^2\} \\
 & = \underset{\Vert \mathbf{w} \Vert = 1}{\arg \max} \{{\Vert Xw \Vert}^2\} \\
 & = \underset{\Vert \mathbf{w} \Vert = 1}{\arg \max} \{ w^TX^TXw \} \\\\
\end{align*}
\]<p>当 \(w_{(1)}\) 为单位向量时，</p>\[w_{(1)} = \arg \max \left\{ \frac{w^TX^TXw}{w^Tw} \right\}
\]<p><strong>瑞利商（Rayleigh quotient）</strong>的定义是：</p>\[R(M, x) = \frac{x^*Mx} {x^*x}
\]<p>如果 \(M\) 矩阵是一个 Hermitian 矩阵，实数范围内就是对称矩阵，那么有 \(R(M, x) \in [\lambda_{min}, \lambda_{max}]\)，如果取最大值，则 \(x\) 为最大的特征值对应的一个特征向量。</p><p>所以 \(w_{(1)}\) 的就是 \(X^TX\) 的最大特征值所对应的特征向量，也就是 \(X\) 的最大奇异值在 \(V\) 中所对应的右奇异向量。</p><p>从 \(X\) 中减去前 \(k - 1\) 个主成分得到 \(\hat{X_k}\)。</p>\[\hat{X_k} = X - \sum_{s = 1}^{k - 1}Xw_{(s)}w_{(s)}^T
\]\[w_{(k)} = \arg \max \left\{ \frac{w^T\hat{X_k}^T\hat{X_k}w}{w^Tw} \right\}
\]<p>经过计算可以得出 \(\hat{X_k}^T\hat{X_k}\) 与 \(X^TX\) 的特征值及特征向量相同，可以得到 \(w_{(k)}\) 是 \(X^TX\) 第 \(k\) 大的特征值对应的特征向量。</p><p>最后可以写成：</p>\[T = XW
\]<p>其中 \(W_{p \times p}\) 是权重矩阵，它的列向量是 \(X^TX\) 的特征向量，\(X\) 是一个 \(n \times p\) 的矩阵，\(n\) 可以代表实验次数，\(p\) 可以代表特征的个数。</p><p>可以只保留前 \(L\) 主成分，得到 \(T_L = X W_L\)，其中 \(T_L\) 是一个 \(n \times L\) 的矩阵，\(W_L\) 是一个 \(p \times L\) 的矩阵。</p><h3>将 SVD 用于 PCA</h3><p>将 \(X\) 做 SVD 得到 \(X = U\Sigma W^T\)，于是 \(T\) 可以写成：</p>\[\begin{align*}
T
&= XW \\
&= U\Sigma W^TW \\
&= U\Sigma
\end{align*}
\]<p>\(T\) 矩阵可以变成左奇异向量乘以对应的奇异值得到，这个叫做<strong>极分解（Polar decomposition）</strong>。</p><p>对于矩阵 \(A\)，如果取出 \(k\) 个最大的奇异值，得到 \(A_k\)。</p>\[A = U \Sigma V^T = \sigma_1u_1v_1^T + \dots + \sigma_ru_rv_r^T \\
A_k = U_k \Sigma_k V_k^T = \sigma_1u_1v_1^T + \dots + \sigma_ku_kv_k^T
\]<p><strong>Eckart–Young 定理</strong>：如果 \(B\) 矩阵的秩为 \(k\)，那么 \(\Vert A - B \Vert \ge \Vert A - A_k \Vert\)。</p><p>被截取的 \(n\times L\) 矩阵 \(T_L\) 可以通过获得 \(L\) 个最大的奇异值及其对应的奇异向量组成。</p>\[T_L = U_L\Sigma_L = X W_L
\]<p>根据 Eckart–Young 定理，通过这种截断的奇异值分解得到的矩阵，\(T_L\) 或者 \(A_L\) 是最接近的原来矩阵的秩为 \(L\) 的矩阵。</p><p>[1] <a href="https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/">Matrix Methods in Data Analysis, Signal Processing, and Machine Learning</a></p><p>[2] <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal component analysis</a></p><p>[3] <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Geometric_meaning">Singular value decomposition</a></p><p>[4] <a href="https://seanwangjs.github.io/2017/11/27/rayleigh-quotient-maximum.html">瑞利商与极值计算</a></p><p>[5] <a href="https://en.wikipedia.org/wiki/Rayleigh_quotient">Rayleigh quotient</a></p><a class="fill-link" href="/math/svd-pca"></a><div class="hide-article-box"></div></div></article></body></html>